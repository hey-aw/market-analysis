{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1f35c7",
   "metadata": {},
   "source": [
    "# Thematic Analysis\n",
    "\n",
    "This notebook contains Python code samples for analyzing interview transcript data, \n",
    "focusing on themes of trust and reliability.\n",
    "\n",
    "We will perform various operations including data loading, filtering, sentiment analysis, \n",
    "keyword identification, and data aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3090b2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        speaker                  theme  \\\n",
      "0           0  Robert.Lehman  Purchasing Experience   \n",
      "1           1  Robert.Lehman   Educational Policies   \n",
      "2           2  Robert.Lehman      Digital Resources   \n",
      "3           3  Robert.Lehman      Budget and Timing   \n",
      "4           4  Robert.Lehman          Buying Habits   \n",
      "\n",
      "                                             context  sentiment_score brand  \\\n",
      "0  Robert mentions the difficulties of purchasing...             -0.4   NaN   \n",
      "1  Robert discusses how educational policies infl...             -0.3   NaN   \n",
      "2  Robert talks about the shift towards digital t...              0.1   NaN   \n",
      "3  Robert explains the budgeting process within t...              0.0   NaN   \n",
      "4  Robert explains his decision to spend out of p...              0.2   NaN   \n",
      "\n",
      "  identified_purchases start_time end_time                    email  ...  \\\n",
      "0                   []      06:04    07:03  robert.lehman@pgcps.org  ...   \n",
      "1                   []      06:04    07:03  robert.lehman@pgcps.org  ...   \n",
      "2                   []      04:49    05:25  robert.lehman@pgcps.org  ...   \n",
      "3                   []      05:44    06:09  robert.lehman@pgcps.org  ...   \n",
      "4                   []      03:21    03:44  robert.lehman@pgcps.org  ...   \n",
      "\n",
      "     BioRad Familiarity   BioCorp Familiarity    Amazon Familiarity  \\\n",
      "0  Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
      "1  Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
      "2  Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
      "3  Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
      "4  Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
      "\n",
      "      Nasco Familiarity Frey/School Specialty Familiarity  \\\n",
      "0  Aware of (don't use)                    Never heard of   \n",
      "1  Aware of (don't use)                    Never heard of   \n",
      "2  Aware of (don't use)                    Never heard of   \n",
      "3  Aware of (don't use)                    Never heard of   \n",
      "4  Aware of (don't use)                    Never heard of   \n",
      "\n",
      "                         Primary Vendor      Top Vendor Qualities  \\\n",
      "0  Carolina Biological,Flinn Scientific  District approved vendor   \n",
      "1  Carolina Biological,Flinn Scientific  District approved vendor   \n",
      "2  Carolina Biological,Flinn Scientific  District approved vendor   \n",
      "3  Carolina Biological,Flinn Scientific  District approved vendor   \n",
      "4  Carolina Biological,Flinn Scientific  District approved vendor   \n",
      "\n",
      "  Years in Eduacation interview_id  \\\n",
      "0           4-9 years           23   \n",
      "1           4-9 years           23   \n",
      "2           4-9 years           23   \n",
      "3           4-9 years           23   \n",
      "4           4-9 years           23   \n",
      "\n",
      "                                             snippet  \n",
      "0  Daylene Long (06:04):\\n\\nOf range are you usua...  \n",
      "1  Daylene Long (06:04):\\n\\nOf range are you usua...  \n",
      "2  Robert.Lehman (04:49):\\n\\nAnd now our county h...  \n",
      "3  Robert.Lehman (05:44):\\n\\nIt gets allocated, e...  \n",
      "4  Robert.Lehman (03:21):\\n\\nReason that I chose ...  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display as dp\n",
    "\n",
    "# Load your data\n",
    "file_path = 'data/annotations.csv'  # Update with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92196bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme</th>\n",
       "      <th>context</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>brand</th>\n",
       "      <th>identified_purchases</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>...</th>\n",
       "      <th>BioRad Familiarity</th>\n",
       "      <th>BioCorp Familiarity</th>\n",
       "      <th>Amazon Familiarity</th>\n",
       "      <th>Nasco Familiarity</th>\n",
       "      <th>Frey/School Specialty Familiarity</th>\n",
       "      <th>Primary Vendor</th>\n",
       "      <th>Top Vendor Qualities</th>\n",
       "      <th>Years in Eduacation</th>\n",
       "      <th>interview_id</th>\n",
       "      <th>snippet_anon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Purchasing Experience</td>\n",
       "      <td>Robert mentions the difficulties of purchasing...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>06:04</td>\n",
       "      <td>07:03</td>\n",
       "      <td>R_2YLFeZ1mpUDw7L9</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Lehman</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Flinn Scientific</td>\n",
       "      <td>District approved vendor</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>23</td>\n",
       "      <td>Interviewer 1 (06:04):\\n\\nOf range are you usu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buying Habits</td>\n",
       "      <td>Robert explains his decision to spend out of p...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>03:21</td>\n",
       "      <td>03:44</td>\n",
       "      <td>R_2YLFeZ1mpUDw7L9</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Lehman</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Flinn Scientific</td>\n",
       "      <td>District approved vendor</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>23</td>\n",
       "      <td>Educator (03:21):\\n\\nReason that I chose to do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vendor Comparison</td>\n",
       "      <td>Flynn has a better selection for the use of Ve...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Flinn Scientific</td>\n",
       "      <td>[ProductPurchaseDetail(product_name='Vernier s...</td>\n",
       "      <td>16:46</td>\n",
       "      <td>17:18</td>\n",
       "      <td>R_2YLFeZ1mpUDw7L9</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Lehman</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Flinn Scientific</td>\n",
       "      <td>District approved vendor</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>23</td>\n",
       "      <td>Educator (16:46):\\n\\nFlynn in my opinion, has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Carolina Purchases</td>\n",
       "      <td>Carolina usually provides the kits, like ones ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Carolina Biological Supply</td>\n",
       "      <td>[ProductPurchaseDetail(product_name='kits cont...</td>\n",
       "      <td>17:18</td>\n",
       "      <td>18:03</td>\n",
       "      <td>R_2YLFeZ1mpUDw7L9</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Lehman</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Flinn Scientific</td>\n",
       "      <td>District approved vendor</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>23</td>\n",
       "      <td>Educator (17:18):\\n\\nCarolina usually are the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Product Quality</td>\n",
       "      <td>He prefers real scientific exploration over th...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>13:54</td>\n",
       "      <td>15:24</td>\n",
       "      <td>R_2YLFeZ1mpUDw7L9</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Lehman</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Flinn Scientific</td>\n",
       "      <td>District approved vendor</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>23</td>\n",
       "      <td>Educator (13:54):\\n\\nWell, the kits, well in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Customer Experience</td>\n",
       "      <td>Has had experiences with kits that were not or...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>44:29</td>\n",
       "      <td>44:29</td>\n",
       "      <td>R_11bIskMPuh55EKW</td>\n",
       "      <td>Gregory</td>\n",
       "      <td>Ruber</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Amazon</td>\n",
       "      <td>District approved vendor,Free shipping (unlimi...</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>9</td>\n",
       "      <td>Educator (44:29):\\n\\nI definitely have ordered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Customer Experience</td>\n",
       "      <td>Desires a website with a collection of accessi...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>47:41</td>\n",
       "      <td>47:41</td>\n",
       "      <td>R_11bIskMPuh55EKW</td>\n",
       "      <td>Gregory</td>\n",
       "      <td>Ruber</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Amazon</td>\n",
       "      <td>District approved vendor,Free shipping (unlimi...</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>9</td>\n",
       "      <td>Educator (47:41):\\n\\nGuess it would just be in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Customer Service</td>\n",
       "      <td>Would utilize a safety video included in a kit...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>50:13</td>\n",
       "      <td>50:13</td>\n",
       "      <td>R_11bIskMPuh55EKW</td>\n",
       "      <td>Gregory</td>\n",
       "      <td>Ruber</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Amazon</td>\n",
       "      <td>District approved vendor,Free shipping (unlimi...</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>9</td>\n",
       "      <td>Educator (50:13):\\n\\nSure. I'd use that, espec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Product Quality</td>\n",
       "      <td>Prefers diversity of results in experiments, a...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>51:20</td>\n",
       "      <td>51:20</td>\n",
       "      <td>R_11bIskMPuh55EKW</td>\n",
       "      <td>Gregory</td>\n",
       "      <td>Ruber</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Amazon</td>\n",
       "      <td>District approved vendor,Free shipping (unlimi...</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>9</td>\n",
       "      <td>Educator (51:20):\\n\\nI mean, obviously I would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>Customer Experience</td>\n",
       "      <td>The teacher enjoys participating in interviews...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>54:54</td>\n",
       "      <td>56:27</td>\n",
       "      <td>R_11bIskMPuh55EKW</td>\n",
       "      <td>Gregory</td>\n",
       "      <td>Ruber</td>\n",
       "      <td>...</td>\n",
       "      <td>Aware of (don't use)</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Current Vendor</td>\n",
       "      <td>Never heard of</td>\n",
       "      <td>Carolina Biological,Amazon</td>\n",
       "      <td>District approved vendor,Free shipping (unlimi...</td>\n",
       "      <td>4-9 years</td>\n",
       "      <td>9</td>\n",
       "      <td>Educator (54:54):\\n\\nThat would be cool. And t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     theme                                            context  \\\n",
       "0    Purchasing Experience  Robert mentions the difficulties of purchasing...   \n",
       "4            Buying Habits  Robert explains his decision to spend out of p...   \n",
       "5        Vendor Comparison  Flynn has a better selection for the use of Ve...   \n",
       "6       Carolina Purchases  Carolina usually provides the kits, like ones ...   \n",
       "7          Product Quality  He prefers real scientific exploration over th...   \n",
       "..                     ...                                                ...   \n",
       "895    Customer Experience  Has had experiences with kits that were not or...   \n",
       "897    Customer Experience  Desires a website with a collection of accessi...   \n",
       "899       Customer Service  Would utilize a safety video included in a kit...   \n",
       "900        Product Quality  Prefers diversity of results in experiments, a...   \n",
       "902    Customer Experience  The teacher enjoys participating in interviews...   \n",
       "\n",
       "     sentiment_score                       brand  \\\n",
       "0               -0.4                         NaN   \n",
       "4                0.2                         NaN   \n",
       "5                0.3            Flinn Scientific   \n",
       "6                0.0  Carolina Biological Supply   \n",
       "7                0.2                         NaN   \n",
       "..               ...                         ...   \n",
       "895             -0.4                         NaN   \n",
       "897              0.8                         NaN   \n",
       "899              0.6                         NaN   \n",
       "900              0.5                         NaN   \n",
       "902              0.8                         NaN   \n",
       "\n",
       "                                  identified_purchases start_time end_time  \\\n",
       "0                                                   []      06:04    07:03   \n",
       "4                                                   []      03:21    03:44   \n",
       "5    [ProductPurchaseDetail(product_name='Vernier s...      16:46    17:18   \n",
       "6    [ProductPurchaseDetail(product_name='kits cont...      17:18    18:03   \n",
       "7                                                   []      13:54    15:24   \n",
       "..                                                 ...        ...      ...   \n",
       "895                                                 []      44:29    44:29   \n",
       "897                                                 []      47:41    47:41   \n",
       "899                                                 []      50:13    50:13   \n",
       "900                                                 []      51:20    51:20   \n",
       "902                                                 []      54:54    56:27   \n",
       "\n",
       "            ResponseId FirstName LastName  ...    BioRad Familiarity  \\\n",
       "0    R_2YLFeZ1mpUDw7L9    Robert   Lehman  ...  Aware of (don't use)   \n",
       "4    R_2YLFeZ1mpUDw7L9    Robert   Lehman  ...  Aware of (don't use)   \n",
       "5    R_2YLFeZ1mpUDw7L9    Robert   Lehman  ...  Aware of (don't use)   \n",
       "6    R_2YLFeZ1mpUDw7L9    Robert   Lehman  ...  Aware of (don't use)   \n",
       "7    R_2YLFeZ1mpUDw7L9    Robert   Lehman  ...  Aware of (don't use)   \n",
       "..                 ...       ...      ...  ...                   ...   \n",
       "895  R_11bIskMPuh55EKW   Gregory    Ruber  ...  Aware of (don't use)   \n",
       "897  R_11bIskMPuh55EKW   Gregory    Ruber  ...  Aware of (don't use)   \n",
       "899  R_11bIskMPuh55EKW   Gregory    Ruber  ...  Aware of (don't use)   \n",
       "900  R_11bIskMPuh55EKW   Gregory    Ruber  ...  Aware of (don't use)   \n",
       "902  R_11bIskMPuh55EKW   Gregory    Ruber  ...  Aware of (don't use)   \n",
       "\n",
       "      BioCorp Familiarity    Amazon Familiarity     Nasco Familiarity  \\\n",
       "0    Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
       "4    Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
       "5    Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
       "6    Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
       "7    Aware of (don't use)  Aware of (don't use)  Aware of (don't use)   \n",
       "..                    ...                   ...                   ...   \n",
       "895        Never heard of        Current Vendor        Current Vendor   \n",
       "897        Never heard of        Current Vendor        Current Vendor   \n",
       "899        Never heard of        Current Vendor        Current Vendor   \n",
       "900        Never heard of        Current Vendor        Current Vendor   \n",
       "902        Never heard of        Current Vendor        Current Vendor   \n",
       "\n",
       "    Frey/School Specialty Familiarity                        Primary Vendor  \\\n",
       "0                      Never heard of  Carolina Biological,Flinn Scientific   \n",
       "4                      Never heard of  Carolina Biological,Flinn Scientific   \n",
       "5                      Never heard of  Carolina Biological,Flinn Scientific   \n",
       "6                      Never heard of  Carolina Biological,Flinn Scientific   \n",
       "7                      Never heard of  Carolina Biological,Flinn Scientific   \n",
       "..                                ...                                   ...   \n",
       "895                    Never heard of            Carolina Biological,Amazon   \n",
       "897                    Never heard of            Carolina Biological,Amazon   \n",
       "899                    Never heard of            Carolina Biological,Amazon   \n",
       "900                    Never heard of            Carolina Biological,Amazon   \n",
       "902                    Never heard of            Carolina Biological,Amazon   \n",
       "\n",
       "                                  Top Vendor Qualities Years in Eduacation  \\\n",
       "0                             District approved vendor           4-9 years   \n",
       "4                             District approved vendor           4-9 years   \n",
       "5                             District approved vendor           4-9 years   \n",
       "6                             District approved vendor           4-9 years   \n",
       "7                             District approved vendor           4-9 years   \n",
       "..                                                 ...                 ...   \n",
       "895  District approved vendor,Free shipping (unlimi...           4-9 years   \n",
       "897  District approved vendor,Free shipping (unlimi...           4-9 years   \n",
       "899  District approved vendor,Free shipping (unlimi...           4-9 years   \n",
       "900  District approved vendor,Free shipping (unlimi...           4-9 years   \n",
       "902  District approved vendor,Free shipping (unlimi...           4-9 years   \n",
       "\n",
       "    interview_id                                       snippet_anon  \n",
       "0             23  Interviewer 1 (06:04):\\n\\nOf range are you usu...  \n",
       "4             23  Educator (03:21):\\n\\nReason that I chose to do...  \n",
       "5             23  Educator (16:46):\\n\\nFlynn in my opinion, has ...  \n",
       "6             23  Educator (17:18):\\n\\nCarolina usually are the ...  \n",
       "7             23  Educator (13:54):\\n\\nWell, the kits, well in p...  \n",
       "..           ...                                                ...  \n",
       "895            9  Educator (44:29):\\n\\nI definitely have ordered...  \n",
       "897            9  Educator (47:41):\\n\\nGuess it would just be in...  \n",
       "899            9  Educator (50:13):\\n\\nSure. I'd use that, espec...  \n",
       "900            9  Educator (51:20):\\n\\nI mean, obviously I would...  \n",
       "902            9  Educator (54:54):\\n\\nThat would be cool. And t...  \n",
       "\n",
       "[599 rows x 37 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trust_themes_df = pd.read_csv('data/trust_themes_anon.csv', index_col=0)\n",
    "trust_themes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a4cd08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Topic 0:': 'like just interviewer know don okay really think going ve',\n",
       " 'Topic 1:': 'interviewer like just amazon okay carolina think order things don',\n",
       " 'Topic 2:': 'science interviewer really like lot teacher school ve stuff just',\n",
       " 'Topic 3:': 'things like interviewer just kind year use going ve okay',\n",
       " 'Topic 4:': 'just don really interviewer like think school know yeah need'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# Extracting the snippets for NLP analysis\n",
    "snippets = trust_themes_df['snippet_anon'].dropna()\n",
    "\n",
    "# Setting up a CountVectorizer for text processing\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "transformed_data = vectorizer.fit_transform(snippets)\n",
    "\n",
    "# Using Latent Dirichlet Allocation for topic modeling\n",
    "n_topics = 5  # Assuming 5 main topics for initial exploration\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "lda.fit(transformed_data)\n",
    "\n",
    "# Function to display top words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d:\" % (topic_idx)] = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    return topic_dict\n",
    "\n",
    "no_top_words = 10\n",
    "topics = display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)\n",
    "\n",
    "topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f6ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328f817d",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5fb0d5",
   "metadata": {},
   "source": [
    "Clean up the text and create a word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "text = ''.join(df['snippet'])\n",
    "def remove_lines_with_timestamps(text):\n",
    "    # Remove lines with Daylene or Kimberly\n",
    "    text = re.sub(r'.*?(Daylene|Kimberly).*\\n?', '', text, flags=re.MULTILINE)\n",
    "    # Remove remaining lines that contain timestamps\n",
    "    # Matches lines with patterns like \"(16:46)\"\n",
    "    return re.sub(r'.*?\\(\\d{2}:\\d{2}\\).*\\n?', '', text, flags=re.MULTILINE).strip()\n",
    "\n",
    "def get_sentences(df):\n",
    "    text = ''.join(df['snippet'])\n",
    "    # Normalize case and remove punctuation\n",
    "    text = remove_lines_with_timestamps(text).lower()\n",
    "    # Remove commas\n",
    "    text = text.replace(',', '')\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove non-alphabetic tokens\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    # Tokenize the text\n",
    "    tokens = sent_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def get_words(df):\n",
    "    text = ''.join(df['snippet'])\n",
    "    # Normalize case and remove punctuation\n",
    "    text = remove_lines_with_timestamps(text).lower()\n",
    "    # Remove commas\n",
    "    text = text.replace(',', '')\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove non-alphabetic tokens\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "words = get_words(df)\n",
    "sentences = get_sentences(df)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency distribution\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Create the frequency distribution of the remaining tokens\n",
    "all_tokens = words\n",
    "fdist = FreqDist(all_tokens)\n",
    "\n",
    "fdist.most_common(10)\n",
    "fdist.plot(10)\n",
    "fdist.tabulate(10)\n",
    "print(f\"Number of unique tokens: {fdist.N()}\")\n",
    "print(f\"Number of documents: {fdist.B()}\")\n",
    "print(f\"Total number of tokens: {fdist.N()}\")\n",
    "print(f\"Lexical diversity: {fdist.B() / fdist.N()}\")\n",
    "print(f\"Lexical density: {fdist.N() / fdist.B()}\")\n",
    "print(f\"Lexical density: {fdist.N() / fdist.B()}\")\n",
    "print(f\"Lexical density: {fdist.N() / fdist.B()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "\n",
    "def get_bigrams_from_df(df):\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(get_words(df))\n",
    "\n",
    "    # Find bigrams that include the word 'trust'\n",
    "    finder.apply_ngram_filter(lambda w1, w2: 'trust' not in (w1.lower(), w2.lower()))\n",
    "    \n",
    "    # Score the bigrams by frequency\n",
    "    scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "    \n",
    "    # Sort highest to lowest based on the score\n",
    "    scoredList = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scoredList\n",
    "\n",
    "bigrams = get_bigrams_from_df(df)\n",
    "bigrams\n",
    "\n",
    "# plot the word and strength of association using nltk and plotly\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigrams)\n",
    "# create separate columns for the bigram tokens\n",
    "bigram_df['word1'] = bigram_df[0].apply(lambda x: x[0])\n",
    "bigram_df['word2'] = bigram_df[0].apply(lambda x: x[1])\n",
    "# rename the columns\n",
    "bigram_df.columns = ['bigram', 'strength', 'word1', 'word2']\n",
    "# drop the bigram column\n",
    "bigram_df.drop(columns='bigram', inplace=True)\n",
    "\n",
    "# create a column with just the associated word, not trust\n",
    "bigram_df['word'] = bigram_df.apply(lambda x: x['word1'] if x['word1'] != 'trust' else x['word2'], axis=1)\n",
    "\n",
    "bigram_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display as word cloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display word cloud with relative importance\n",
    "wordcloud = WordCloud().generate_from_frequencies(fdist)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eab3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sentiment_by_speaker = df.groupby('speaker')['sentiment_score'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(average_sentiment_by_speaker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-analysis-OkwLYUTU-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
