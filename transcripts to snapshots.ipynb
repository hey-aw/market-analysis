{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = st.secrets.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__b060afcb95f84771b911bb20e5955706\"\n",
    "# os.environ [\"LANGCHAIN_PROJECT\"] = \"My Project Name\" # Optional: \"default\" is used if not set\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "TRANSCRIPT_PATH = \"data/transcripts\"\n",
    "ANNOTATIONS_PATH = \"data/annotations\"\n",
    "ASSISTANT_ID = \"asst_XfSqQlAPl7opg5fsMCjE9lfL\"\n",
    "DATA_FILES = os.listdir(\"data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import new transcripts from data directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "new_transcripts = []\n",
    "\n",
    "def load_transcripts_from_folder(folder_path):\n",
    "    # create a dataframe with the file name and the transcript\n",
    "    transcripts = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.docx'):  # Process only .docx files\n",
    "            doc = Docx2txtLoader(os.path.join(folder_path, file))\n",
    "            transcript = doc.load()[0].page_content\n",
    "            transcripts.append({\"file\": file, \"transcript\": transcript})\n",
    "    return transcripts\n",
    "\n",
    "# Create DataFrame for the new transcripts\n",
    "new_transcripts_df = pd.DataFrame(load_transcripts_from_folder(DATA_PATH), columns=[\"file\", \"transcript\"])\n",
    "new_transcripts_df.head(5)\n",
    "\n",
    "# move the new transcripts to the transcripts folder\n",
    "for file in new_transcripts_df['file']:\n",
    "    os.rename(os.path.join(DATA_PATH, file), os.path.join(TRANSCRIPT_PATH, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all transcripts with survey data and create a dataframe with all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "survey_data = pd.read_csv(\"data/survey_data.csv\")\n",
    "\n",
    "def merge_transcripts_with_surveys(transcripts_df, survey_data):\n",
    "    dataframe = transcripts_df.copy()\n",
    "    for index, row in transcripts_df.iterrows():\n",
    "        file_name = row['file']\n",
    "        transcript = row['transcript']\n",
    "    \n",
    "        # Separate keywords with whitespace characters, hyphens, or underscores\n",
    "        keywords = re.findall(r\"[\\w'-]+\", file_name)\n",
    "        \n",
    "        # filter keywords to first two name-sized words\n",
    "        keywords = [keyword for keyword in keywords if len(keyword) >= 3]\n",
    "        first_name = keywords[0]\n",
    "        last_name = keywords[1]\n",
    "        \n",
    "        # find survey data for this person by filtering each name\n",
    "        filtered_data = survey_data[survey_data['RecipientFirstName'].str.contains(first_name, case=False) & survey_data['RecipientLastName'].str.contains(last_name, case=False)]\n",
    "        \n",
    "        if filtered_data.empty:\n",
    "            print(f\"Could not find survey data for {first_name} {last_name}\")\n",
    "            continue\n",
    "\n",
    "        # add survey data to transcript dataframe: Age Group, Email, Role\n",
    "        dataframe.loc[index, 'ResponseId'] = filtered_data['ResponseId'].values[0]\n",
    "        dataframe.loc[index, 'FirstName'] = filtered_data['RecipientFirstName'].values[0]\n",
    "        dataframe.loc[index, 'LastName'] = filtered_data['RecipientLastName'].values[0]\n",
    "        dataframe.loc[index, 'Email'] = filtered_data['RecipientEmail'].values[0]\n",
    "        dataframe.loc[index, 'AgeGroup'] = filtered_data['Q15'].values[0]\n",
    "        dataframe.loc[index, 'InstitutionName'] = filtered_data['InstitutionName'].values[0]\n",
    "        dataframe.loc[index, 'District'] = filtered_data['ParentName'].values[0]\n",
    "        dataframe.loc[index, 'City'] = filtered_data['MailingCity'].values[0]\n",
    "        dataframe.loc[index, 'State'] = filtered_data['MailingState'].values[0]\n",
    "        dataframe.loc[index, 'Role'] = filtered_data['Q2'].values[0]\n",
    "        dataframe.loc[index, 'Subjects'] = filtered_data['Q3'].values[0]\n",
    "        dataframe.loc[index, 'Courses'] = filtered_data['Q4'].values[0]\n",
    "        dataframe.loc[index, 'TopOfMind'] = filtered_data['Q5'].values[0]\n",
    "        dataframe.loc[index, 'Carolina Familiarity'] = filtered_data['Q6_1'].values[0]\n",
    "        dataframe.loc[index, 'Fisher Familiarity'] = filtered_data['Q6_2'].values[0]\n",
    "        dataframe.loc[index, 'Flinn Scientific Familiarity'] = filtered_data['Q6_3'].values[0]\n",
    "        dataframe.loc[index, 'PLTW Familiarity'] = filtered_data['Q6_4'].values[0]\n",
    "        dataframe.loc[index, 'Sargent Welch Familiarity'] = filtered_data['Q6_5'].values[0]\n",
    "        dataframe.loc[index, 'Thomas Scientific Familiarity'] = filtered_data['Q6_6'].values[0]\n",
    "        dataframe.loc[index, 'Wards/VWR Familiarity'] = filtered_data['Q6_7'].values[0]\n",
    "        dataframe.loc[index, 'BioRad Familiarity'] = filtered_data['Q6_8'].values[0]\n",
    "        dataframe.loc[index, 'BioCorp Familiarity'] = filtered_data['Q6_9'].values[0]\n",
    "        dataframe.loc[index, 'Amazon Familiarity'] = filtered_data['Q6_10'].values[0]\n",
    "        dataframe.loc[index, 'Nasco Familiarity'] = filtered_data['Q6_11'].values[0]\n",
    "        dataframe.loc[index, 'Frey/School Specialty Familiarity'] = filtered_data['Q6_12'].values[0]\n",
    "        dataframe.loc[index, 'Primary Vendor'] = filtered_data['Q7'].values[0]\n",
    "        dataframe.loc[index, 'Top Vendor Qualities'] = filtered_data['Q8'].values[0]\n",
    "        dataframe.loc[index, 'Years in Eduacation'] = filtered_data['Q14'].values[0]\n",
    "    return dataframe\n",
    "\n",
    "# Merge the new transcripts with the survey data\n",
    "new_transcripts_merged_df = merge_transcripts_with_surveys(new_transcripts_df, survey_data)\n",
    "\n",
    "# Save the new transcripts dataframe to a CSV file\n",
    "new_transcripts_merged_df.to_csv(\"data/new_transcripts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the survey data\n",
    "\n",
    "survey_data = pd.read_csv(\"data/survey_data.csv\")\n",
    "\n",
    "# Load data from the transcripts folder\n",
    "transcripts = load_transcripts_from_folder(TRANSCRIPT_PATH)\n",
    "transcripts_df = pd.DataFrame(transcripts, columns=[\"file\", \"transcript\"])\n",
    "\n",
    "# Merge the survey data with the transcripts\n",
    "all_transcripts = merge_transcripts_with_surveys(transcripts_df, survey_data)\n",
    "\n",
    "# Save the merged transcripts to a CSV file\n",
    "all_transcripts.to_csv(\"data/merged_transcripts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process new interview snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_runnable,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "instructions = \"\"\"As the Interview Snapshot Compiler, my role is to assist Carolina Biological Supply Company in their market research by creating interview snapshots from primary sources. I output markdown in the following format: \n",
    "## [Interviewee Name], [Institution Name], [City, State]\n",
    "\n",
    "Category: [Carolina loyalist | Flinn loyalist | Carolina + Flinn | Other]  \n",
    "Generation: [Generation name and year range]\n",
    "\n",
    "### Quick Facts\n",
    "- **Position:** [Position]\n",
    "- **Teaching Areas:** [Teaching Areas]\n",
    "- **Background:** [Background]\n",
    "- **School Type:** [School Type]\n",
    "- **Purchasing Role:** [Purchasing Role]\n",
    "- **Unique Fact:** [Unique Fact]\n",
    "\n",
    "### Memorable Quote\n",
    "- \"[Memorable quote]\" [timestamp] ([brief context])\n",
    "- \"[Memorable quote]\" [timestamp] ([brief context])\n",
    "- \"[Memorable quote]\" [timestamp] ([brief context])\n",
    "\n",
    "### Buyerâ€™s Journey\n",
    "- [Brief notes from each step of this buyer's journey such as: Identification of Needs, Research and Consideration, Decision-Making, Vendor Selection, Post-Purchase Evaluation]\n",
    "\n",
    "### Insights\n",
    "- [Insight from interview]\n",
    "\n",
    "### Opportunities\n",
    "- [Opportunity/need identified]\n",
    "\n",
    "\n",
    "### Video, Transcript & Survey Responses\n",
    "- [to be added later]\n",
    "\"\"\"\n",
    "\n",
    "# Chat Prompt Template from instructions\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\", instructions),\n",
    "      (\"human\", \"Process transcript for email: {email} transcript:\\n {text}\"),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of transcripts to process, skipping any with csv and md files already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sean.taylor@sbcusd.k12.ca.us',\n",
       " 'pqmcgee@madison.k12.wi.us',\n",
       " 'markniebojeski@paps.net']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a list of rows to process\n",
    "rows_to_process = list(new_transcripts_df.iterrows())\n",
    "\n",
    "# filter out rows that have already been processed\n",
    "for index, row in new_transcripts_df.iterrows():\n",
    "    participant_email = row['Email']\n",
    "    md_file_path = f\"{ANNOTATIONS_PATH}/{participant_email}.md\"\n",
    "    csv_file_path = f\"{ANNOTATIONS_PATH}/{participant_email}.csv\"\n",
    "    if os.path.exists(csv_file_path) and os.path.exists(md_file_path):\n",
    "        rows_to_process = [r for r in rows_to_process if r[0] != index]\n",
    "        \n",
    "\n",
    "def process_snapshot(row):\n",
    "    transcript = row[1]['transcript']\n",
    "    participant_email = row[1]['Email']\n",
    "    print(f\"Processing transcript for {participant_email}\")\n",
    "    # Run the chain\n",
    "    result = chain.invoke({\"email\": participant_email, \"text\": transcript})\n",
    "    # Save the results to an md file\n",
    "    markdown = result.content\n",
    "    md_file_path = f\"{ANNOTATIONS_PATH}/{participant_email}.md\"\n",
    "    with open(md_file_path, \"w\") as f:\n",
    "        f.write(markdown)\n",
    "    return markdown\n",
    "snapshots_to_process = rows_to_process\n",
    "# display the emails of the rows to process\n",
    "emails_to_process = [r[1]['Email'] for r in rows_to_process]\n",
    "emails_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the interview snapshots and output md files for each snapshot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to process snapshots one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transcript for sean.taylor@sbcusd.k12.ca.us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Sean Taylor, Sierra High School, San Bernardino, CA\\n\\nCategory: Other  \\nGeneration: Millennial (1981-1996)\\n\\n### Quick Facts\\n- **Position:** Biology Teacher and Department Chair\\n- **Teaching Areas:** Biology, Coding, Forensic Science\\n- **Background:** Teaching since 2009, focus on skill development, claim evidence, and reasoning\\n- **School Type:** Continuation High School\\n- **Purchasing Role:** Department Chair with autonomy in purchasing decisions\\n- **Unique Fact:** School has a focus on mental and emotional health, and building relationships is a priority.\\n\\n### Memorable Quote\\n- \"Our kids are typically with us for six weeks to try to catch up on credits as opposed to the normal 18.\" [01:48] (Discussing the unique challenges of teaching at a continuation high school)\\n- \"I currently teach biology mostly, but I\\'ve been doing a little bit of coding, a little bit of forensic science from here and there...\" [01:48] (Describing his teaching areas)\\n- \"I want every kid to be like, yeah, yeah,\" [36:09] (Expressing his desire to spark curiosity and engagement in his students)\\n\\n### Buyerâ€™s Journey\\n- **Identification of Needs:** Recognizes the need for hands-on materials and real-world applications to engage students.\\n- **Research and Consideration:** Attends conferences like NSTA, browses product brochures, and considers ease of ordering and relevance to curriculum.\\n- **Decision-Making:** Chooses materials based on the potential to teach skills and engage students, such as mini PCR machines.\\n- **Vendor Selection:** Has ordered from Flinn and Carolina in the past but is currently open to any vendor that meets his needs.\\n- **Post-Purchase Evaluation:** Evaluates the success of materials based on student engagement and the ability to teach critical thinking and real-world applications.\\n\\n### Insights\\n- Sean Taylor seeks materials that can be completed within a short timeframe (one to two class periods) and that are flexible enough to cater to a wide range of student abilities and backgrounds.\\n- He values hands-on materials that can help build students\\' confidence and self-esteem, as well as their ability to think critically and understand the world around them.\\n\\n### Opportunities\\n- There is a need for life science materials that align closely with NGSS standards and the Living Earth model, which integrates various scientific disciplines.\\n- Vendors could improve by organizing their offerings according to NGSS standards and essential topics, making it easier for educators to find relevant materials.\\n\\n### Video, Transcript & Survey Responses\\n- [to be added later]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process first snapshot and remove it from the list\n",
    "snapshot = process_snapshot(snapshots_to_process[0])\n",
    "snapshots_to_process = snapshots_to_process[1:]\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to process the remaining snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transcript for pqmcgee@madison.k12.wi.us\n",
      "Processing transcript for markniebojeski@paps.net\n"
     ]
    }
   ],
   "source": [
    "# process the rest of the snapshots\n",
    "for snapshot in snapshots_to_process:\n",
    "    process_snapshot(snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitch together snapshot markdown files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch the md files together into a single md file\n",
    "\n",
    "SNAPSHOTS_DIR = \"data/snapshots/\"\n",
    "\n",
    "md_files = os.listdir(SNAPSHOTS_DIR)\n",
    "\n",
    "# sort the files by email\n",
    "md_files.sort(key=lambda f: f.split(\".\")[0])\n",
    "\n",
    "# read the files into a list\n",
    "mds = []\n",
    "for md_file in md_files:\n",
    "    with open(f\"{SNAPSHOTS_DIR}/{md_file}\", \"r\") as f:\n",
    "        mds.append(f.read())\n",
    "\n",
    "# join the list into a single string\n",
    "md = \"\\n\\n\".join(mds)\n",
    "\n",
    "# write the string to a file\n",
    "with open(\"data/snapshots.md\", \"w\") as f:\n",
    "    f.write(md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process interview snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run manually to process the next transcript until all are processed\n",
    "def process_next_transcript():\n",
    "    # get the next transcript\n",
    "    row = next(rows_to_process)\n",
    "    process_transcript(row)\n",
    "    \n",
    "process_next_transcript()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-analysis-OkwLYUTU-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
